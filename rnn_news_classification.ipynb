{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把源代码文件放入响应名称的文件夹，例如java代码放入java文件夹。然后用ext-cp.py和build-sent-datasets.py构建测试数据。最后生成两个文件，test.csv和test.csv。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20857lines [00:52, 397.79lines/s]\n",
      "20857lines [01:37, 214.12lines/s]\n",
      "10429lines [00:47, 218.59lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "NGRAMS = 2\n",
    "import os\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./data/lang_codes', ngrams=NGRAMS, vocab=None)\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1].long() for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "#     print(text)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 58 seconds\n",
      "\tLoss: 0.0005(train)\t|\tAcc: 99.8%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 99.9%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 59 seconds\n",
      "\tLoss: 0.0004(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 99.9%(valid)\n",
      "Epoch: 3  | time in 1 minutes, 0 seconds\n",
      "\tLoss: 0.0004(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 100.0%(valid)\n",
      "Epoch: 4  | time in 1 minutes, 2 seconds\n",
      "\tLoss: 0.0003(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 99.9%(valid)\n",
      "Epoch: 5  | time in 1 minutes, 1 seconds\n",
      "\tLoss: 0.0002(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 99.8%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train_func(sub_train_)\n",
    "        valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "        secs = int(time.time() - start_time)\n",
    "        mins = secs / 60\n",
    "        secs = secs % 60\n",
    "\n",
    "        print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "        print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "        print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "    except Exception as ex:\n",
    "        print('Epoch: %d' %(epoch + 1), ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0000(test)\t|\tAcc: 99.8%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a cpp \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"cpp\",\n",
    "                 2 : \"java\",\n",
    "                 3 : \"python\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = '''\n",
    "\n",
    "static CPLErr OGRPolygonContourWriter( double dfLevelMin, double dfLevelMax,\n",
    "                                const OGRMultiPolygon& multipoly,\n",
    "                                void *pInfo )\n",
    "\n",
    "{\n",
    "    OGRContourWriterInfo *poInfo = static_cast<OGRContourWriterInfo *>(pInfo);\n",
    "\n",
    "    OGRFeatureDefnH hFDefn =\n",
    "        OGR_L_GetLayerDefn( static_cast<OGRLayerH>(poInfo->hLayer) );\n",
    "\n",
    "    OGRFeatureH hFeat = OGR_F_Create( hFDefn );\n",
    "\n",
    "    if( poInfo->nIDField != -1 )\n",
    "        OGR_F_SetFieldInteger( hFeat, poInfo->nIDField, poInfo->nNextID++ );\n",
    "\n",
    "    if( poInfo->nElevFieldMin != -1 )\n",
    "        OGR_F_SetFieldDouble( hFeat, poInfo->nElevFieldMin, dfLevelMin );\n",
    "\n",
    "    if( poInfo->nElevFieldMax != -1 )\n",
    "        OGR_F_SetFieldDouble( hFeat, poInfo->nElevFieldMax, dfLevelMax );\n",
    "\n",
    "    const bool bHasZ = wkbHasZ(OGR_FD_GetGeomType(hFDefn));\n",
    "    OGRGeometryH hGeom = OGR_G_CreateGeometry(\n",
    "        bHasZ ? wkbMultiPolygon25D : wkbMultiPolygon );\n",
    "\n",
    "    for ( int iPart = 0; iPart < multipoly.getNumGeometries(); iPart++ )\n",
    "    {\n",
    "        OGRPolygon* poNewPoly = new OGRPolygon();\n",
    "        const OGRPolygon* poPolygon = static_cast<const OGRPolygon*>(multipoly.getGeometryRef(iPart));\n",
    "\n",
    "        for ( int iRing = 0; iRing < poPolygon->getNumInteriorRings() + 1; iRing++ )\n",
    "        {\n",
    "            const OGRLinearRing* poRing = iRing == 0 ?\n",
    "                poPolygon->getExteriorRing()\n",
    "                : poPolygon->getInteriorRing(iRing - 1);\n",
    "\n",
    "            OGRLinearRing* poNewRing = new OGRLinearRing();\n",
    "            for ( int iPoint = 0; iPoint < poRing->getNumPoints(); iPoint++ )\n",
    "            {\n",
    "                const double dfX = poInfo->adfGeoTransform[0]\n",
    "                    + poInfo->adfGeoTransform[1] * poRing->getX(iPoint)\n",
    "                    + poInfo->adfGeoTransform[2] * poRing->getY(iPoint);\n",
    "                const double dfY = poInfo->adfGeoTransform[3]\n",
    "                    + poInfo->adfGeoTransform[4] * poRing->getX(iPoint)\n",
    "                    + poInfo->adfGeoTransform[5] * poRing->getY(iPoint);\n",
    "                if( bHasZ )\n",
    "                    OGR_G_SetPoint( OGRGeometry::ToHandle( poNewRing ), iPoint, dfX, dfY, dfLevelMax );\n",
    "                else\n",
    "                    OGR_G_SetPoint_2D( OGRGeometry::ToHandle( poNewRing ), iPoint, dfX, dfY );\n",
    "            }\n",
    "            poNewPoly->addRingDirectly( poNewRing );\n",
    "        }\n",
    "        OGR_G_AddGeometryDirectly( hGeom, OGRGeometry::ToHandle( poNewPoly ) );\n",
    "    }\n",
    "\n",
    "    OGR_F_SetGeometryDirectly( hFeat, hGeom );\n",
    "\n",
    "    const OGRErr eErr =\n",
    "        OGR_L_CreateFeature(static_cast<OGRLayerH>(poInfo->hLayer), hFeat);\n",
    "    OGR_F_Destroy( hFeat );\n",
    "\n",
    "    return eErr == OGRERR_NONE ? CE_None : CE_Failure;\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s \" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
