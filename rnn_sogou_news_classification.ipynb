{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text Classification with TorchText\n",
    "==================================\n",
    "\n",
    "This tutorial shows how to use the text classification datasets\n",
    "in ``torchtext``, including\n",
    "\n",
    "::\n",
    "\n",
    "   - AG_NEWS,\n",
    "   - SogouNews,\n",
    "   - DBpedia,\n",
    "   - YelpReviewPolarity,\n",
    "   - YelpReviewFull,\n",
    "   - YahooAnswers,\n",
    "   - AmazonReviewPolarity,\n",
    "   - AmazonReviewFull\n",
    "\n",
    "This example shows how to train a supervised learning algorithm for\n",
    "classification using one of these ``TextClassification`` datasets.\n",
    "\n",
    "Load data with ngrams\n",
    "---------------------\n",
    "\n",
    "A bag of ngrams feature is applied to capture some partial information\n",
    "about the local word order. In practice, bi-gram or tri-gram are applied\n",
    "to provide more benefits as word groups than only one word. An example:\n",
    "\n",
    "::\n",
    "\n",
    "   \"load data with ngrams\"\n",
    "   Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n",
    "   Tri-grams results: \"load data with\", \"data with ngrams\"\n",
    "\n",
    "``TextClassification`` Dataset supports the ngrams method. By setting\n",
    "ngrams to 2, the example text in the dataset will be a list of single\n",
    "words plus bi-grams string.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450000lines [05:08, 1457.65lines/s]\n",
      "450000lines [10:42, 700.62lines/s] \n",
      "60000lines [01:15, 798.88lines/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "NGRAMS = 2\n",
    "import os\n",
    "train_dataset, test_dataset = text_classification.DATASETS['SogouNews'](\n",
    "    root='./data/sogou_news_csv', ngrams=NGRAMS, vocab=None)\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "The model is composed of the\n",
    "`EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`__\n",
    "layer and the linear layer (see the figure below). ``nn.EmbeddingBag``\n",
    "computes the mean value of a “bag” of embeddings. The text entries here\n",
    "have different lengths. ``nn.EmbeddingBag`` requires no padding here\n",
    "since the text lengths are saved in offsets.\n",
    "\n",
    "Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n",
    "the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n",
    "performance and memory efficiency to process a sequence of tensors.\n",
    "\n",
    "![](../_static/img/text_sentiment_ngrams_model.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an instance\n",
    "--------------------\n",
    "\n",
    "The AG_NEWS dataset has four labels and therefore the number of classes\n",
    "is four.\n",
    "\n",
    "::\n",
    "\n",
    "   1 : World\n",
    "   2 : Sports\n",
    "   3 : Business\n",
    "   4 : Sci/Tec\n",
    "\n",
    "The vocab size is equal to the length of vocab (including single word\n",
    "and ngrams). The number of classes is equal to the number of labels,\n",
    "which is four in AG_NEWS case.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to generate batch\n",
    "--------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the text entries have different lengths, a custom function\n",
    "generate_batch() is used to generate data batches and offsets. The\n",
    "function is passed to ``collate_fn`` in ``torch.utils.data.DataLoader``.\n",
    "The input to ``collate_fn`` is a list of tensors with the size of\n",
    "batch_size, and the ``collate_fn`` function packs them into a\n",
    "mini-batch. Pay attention here and make sure that ``collate_fn`` is\n",
    "declared as a top level def. This ensures that the function is available\n",
    "in each worker.\n",
    "\n",
    "The text entries in the original data batch input are packed into a list\n",
    "and concatenated as a single tensor as the input of ``nn.EmbeddingBag``.\n",
    "The offsets is a tensor of delimiters to represent the beginning index\n",
    "of the individual sequence in the text tensor. Label is a tensor saving\n",
    "the labels of individual text entries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to train the model and evaluate results.\n",
    "---------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n",
    "is recommended for PyTorch users, and it makes data loading in parallel\n",
    "easily (a tutorial is\n",
    "`here <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`__).\n",
    "We use ``DataLoader`` here to load AG_NEWS datasets and send it to the\n",
    "model for training/validation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset and run the model\n",
    "-----------------------------------\n",
    "\n",
    "Since the original AG_NEWS has no valid dataset, we split the training\n",
    "dataset into train/valid sets with a split ratio of 0.95 (train) and\n",
    "0.05 (valid). Here we use\n",
    "`torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>`__\n",
    "function in PyTorch core library.\n",
    "\n",
    "`CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
    "criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\n",
    "It is useful when training a classification problem with C classes.\n",
    "`SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>`__\n",
    "implements stochastic gradient descent method as optimizer. The initial\n",
    "learning rate is set to 4.0.\n",
    "`StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>`__\n",
    "is used here to adjust the learning rate through epochs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 8 minutes, 11 seconds\n",
      "\tLoss: 0.0128(train)\t|\tAcc: 93.8%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 95.2%(valid)\n",
      "Epoch: 2  | time in 8 minutes, 30 seconds\n",
      "\tLoss: 0.0079(train)\t|\tAcc: 96.2%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 96.0%(valid)\n",
      "Epoch: 3  | time in 7 minutes, 45 seconds\n",
      "\tLoss: 0.0065(train)\t|\tAcc: 96.8%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 96.0%(valid)\n",
      "Epoch: 4  | time in 7 minutes, 45 seconds\n",
      "\tLoss: 0.0056(train)\t|\tAcc: 97.3%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 96.0%(valid)\n",
      "Epoch: 5  | time in 8 minutes, 44 seconds\n",
      "\tLoss: 0.0048(train)\t|\tAcc: 97.6%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 95.9%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on GPU with the following information:\n",
    "\n",
    "Epoch: 1 \\| time in 0 minutes, 11 seconds\n",
    "\n",
    "::\n",
    "\n",
    "       Loss: 0.0263(train)     |       Acc: 84.5%(train)\n",
    "       Loss: 0.0001(valid)     |       Acc: 89.0%(valid)\n",
    "\n",
    "\n",
    "Epoch: 2 \\| time in 0 minutes, 10 seconds\n",
    "\n",
    "::\n",
    "\n",
    "       Loss: 0.0119(train)     |       Acc: 93.6%(train)\n",
    "       Loss: 0.0000(valid)     |       Acc: 89.6%(valid)\n",
    "\n",
    "\n",
    "Epoch: 3 \\| time in 0 minutes, 9 seconds\n",
    "\n",
    "::\n",
    "\n",
    "       Loss: 0.0069(train)     |       Acc: 96.4%(train)\n",
    "       Loss: 0.0000(valid)     |       Acc: 90.5%(valid)\n",
    "\n",
    "\n",
    "Epoch: 4 \\| time in 0 minutes, 11 seconds\n",
    "\n",
    "::\n",
    "\n",
    "       Loss: 0.0038(train)     |       Acc: 98.2%(train)\n",
    "       Loss: 0.0000(valid)     |       Acc: 90.4%(valid)\n",
    "\n",
    "\n",
    "Epoch: 5 \\| time in 0 minutes, 11 seconds\n",
    "\n",
    "::\n",
    "\n",
    "       Loss: 0.0022(train)     |       Acc: 99.0%(train)\n",
    "       Loss: 0.0000(valid)     |       Acc: 91.0%(valid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with test dataset\n",
    "------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0000(test)\t|\tAcc: 96.0%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results of test dataset…\n",
    "\n",
    "::\n",
    "\n",
    "       Loss: 0.0237(test)      |       Acc: 90.5%(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a random news\n",
    "---------------------\n",
    "\n",
    "Use the best model so far and test a golf news. The label information is\n",
    "available\n",
    "`here <https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a technology news\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from xpinyin import Pinyin\n",
    "p = Pinyin()\n",
    "\n",
    "\n",
    "ag_news_label = {1 : \"sports\",\n",
    "                 2 : \"finance\",\n",
    "                 3 : \"entertainment\",\n",
    "                 4 : \"automobile\",\n",
    "                 5 : \"technology\"\n",
    "                }\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    text = p.get_pinyin(text, ' ', tone_marks='numbers')\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = '''\n",
    "昨天，湖南卫视也举行一场全民狂欢的购物晚会，同时请来了不少当红明星为其助阵。喜剧达人沈腾也在其中。近几年，沈腾凭借多部喜剧电影的大火，得到人们的喜爱，喜剧上的天赋也得到了网友的肯定。以至于人们说沈腾恐怕是长在我们笑点上了，他的一举一动，一言一行，都能逗得大家哈哈笑。\n",
    "沈腾以前在节目上说过他当过军艺的校草，而如今的一线男明星杨洋也是军艺的校草。当在台下的沈腾看着杨洋缓缓走上舞台时，沈腾的面部表情都控制不住了，真的是老乡见老乡，两眼泪汪汪啊。\n",
    "晚会当天，杨洋的舞台造型也让人眼前一亮，身穿黑色风衣搭配假发和贝雷帽，着实抢眼。相比之下，沈腾的穿着就比较中规中矩了，西装配领带。当看到表演完从台上下来的杨洋时，我们的沈腾老师捂住嘴巴，已经快控制不住自己的情绪了。当杨洋走到沈腾面前时，两人便紧紧的抱在一起，如失散多年的兄弟一样亲昵。\n",
    "\n",
    "'''\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Sports news\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the code examples displayed in this note\n",
    "`here <https://github.com/pytorch/text/tree/master/examples/text_classification>`__.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
